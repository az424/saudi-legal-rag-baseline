# rag_pipeline.py
import os
import json
import gc
from dotenv import load_dotenv
from openai import OpenAI
import cohere

from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_openai import OpenAIEmbeddings

# =====================
# Environment
# =====================
load_dotenv()

DATA_PATH = "full_systems_dataset_fixed.json"
EMBED_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
GPT_MODEL = os.getenv("OPENAI_MODEL", "gpt-4-turbo")
COHERE_KEY = os.getenv("COHERE_API_KEY")

client = OpenAI()

co = None
if COHERE_KEY:
    try:
        co = cohere.Client(COHERE_KEY)
    except Exception:
        print("[RAG] Cohere init failed.")

# =====================
# Global (Lazy Load)
# =====================
_faiss_retriever = None
_bm25_retriever = None

# =====================
# Dataset & Retrievers
# =====================
def initialize_retrievers():
    global _faiss_retriever, _bm25_retriever
    if _faiss_retriever is not None:
        return _faiss_retriever, _bm25_retriever

    print("[RAG] Loading dataset...")
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)

    documents = []
    for item in data:
        full_text = (
            f"Ø§Ù„Ù†Ø¸Ø§Ù…: {item['system']}\n"
            f"Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø©: {item['article_number']}\n"
            f"Ø§Ù„Ù†Øµ: {item['text']}"
        )
        metadata = {
            "system": item["system"],
            "article_number": item["article_number"],
            "article_key": item.get("article_key", item["article_number"]),
        }
        documents.append(Document(page_content=full_text, metadata=metadata))

    del data
    gc.collect()

    embeddings = OpenAIEmbeddings(model=EMBED_MODEL)
    faiss_store = FAISS.from_documents(documents, embeddings)

    _faiss_retriever = faiss_store.as_retriever(search_kwargs={"k": 30})
    _bm25_retriever = BM25Retriever.from_documents(documents)
    _bm25_retriever.k = 30

    del documents
    gc.collect()

    print("[RAG] Retrievers ready.")
    return _faiss_retriever, _bm25_retriever

# =====================
# Query Optimization
# =====================
def optimize_query_for_legal_search(query: str) -> str:
    """
    Force retrieval of governing procedural rules
    """
    return f"""
{query}
ØªØ­Ø±ÙŠØ± Ø§Ù„Ø¯Ø¹ÙˆÙ‰
Ø¹Ø¯Ù… Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ø¯Ø¹ÙˆÙ‰
Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø©
Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª Ø§Ù„Ø´Ø±Ø¹ÙŠØ©
Ø§Ù„Ù…Ø§Ø¯Ø© 66
"""

# =====================
# Prompt Builder (CRITICAL)
# =====================
def build_prompt(question: str, docs: list[Document]) -> str:
    context = "\n\n".join(
        f"--- Ø§Ù„Ù…Ø±Ø¬Ø¹ {i+1} ---\n{d.page_content}"
        for i, d in enumerate(docs)
    )

    return f"""
Ø£Ù†Øª Ù‚Ø§Ø¶Ù ØªØ¬Ø§Ø±ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ.

Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ **Ø­ØµØ±ÙŠÙ‹Ø§** Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ø£Ø¯Ù†Ø§Ù‡ØŒ Ø¯ÙˆÙ† Ø£ÙŠ Ø§Ø¬ØªÙ‡Ø§Ø¯ Ø®Ø§Ø±Ø¬ÙŠ.

Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ©:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„:
{question}

âš ï¸ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¥Ù„Ø²Ø§Ù…ÙŠØ©:
1. Ù…ÙŠÙ‘Ø² Ø¨ÙŠÙ† Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© (Ø§Ù„Ù†Ø¸Ø§Ù…) ÙˆØ§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ø®Ø§ØµØ© Ø£Ùˆ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ© (Ø§Ù„Ù„Ø§Ø¦Ø­Ø©).
2. Ù„Ø§ ØªØ·Ø¨Ù‚ Ù†ØµÙ‹Ø§ Ø®Ø§ØµÙ‹Ø§ Ø¹Ù„Ù‰ Ù…Ø³Ø£Ù„Ø© ØªØ­ÙƒÙ…Ù‡Ø§ Ù‚Ø§Ø¹Ø¯Ø© Ø¹Ø§Ù…Ø©.
3. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø³Ø£Ù„Ø© ØªØªØ¹Ù„Ù‚ Ø¨ØªØ­Ø±ÙŠØ± Ø§Ù„Ø¯Ø¹ÙˆÙ‰ØŒ ÙÙ„Ø§ ØªØ·Ø¨Ù‚ Ù†ØµÙˆØµ "Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ø·Ù„Ø¨Ø§Øª".
4. Ø¥Ø°Ø§ Ù„Ù… ØªØªØ¶Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ø§Ù„Ø­Ø§ÙƒÙ…Ø©ØŒ **ÙŠØ¬Ø¨ Ø£Ù† ØªØµØ±Ù‘Ø­ Ø¨Ø¹Ø¯Ù… ÙƒÙØ§ÙŠØ© Ø§Ù„Ù†ØµÙˆØµ**.
5. Ù„Ø§ ØªØ¯Ø®Ù„ ÙÙŠ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹ÙŠØ¨ Ø´ÙƒÙ„ÙŠÙ‹Ø§.

ØµÙŠØºØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
- Ø§Ù„Ø­ÙƒÙ… Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠ (ÙŠØ¬ÙˆØ² / Ù„Ø§ ÙŠØ¬ÙˆØ²)
- Ø§Ù„Ø£Ø«Ø± Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠ (Ø¹Ø¯Ù… Ù‚Ø¨ÙˆÙ„ / ØµØ±Ù Ù†Ø¸Ø±)
- Ø°ÙƒØ± Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø§Ù„ØµØ­ÙŠØ­Ø© ÙÙ‚Ø·
""".strip()

# =====================
# Validation Layer
# =====================
def has_governing_rule(docs: list[Document]) -> bool:
    """
    Ensure presence of general procedural law
    """
    for d in docs:
        if "Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª" in d.metadata.get("system", ""):
            return True
    return False

# =====================
# Retrieval Logic
# =====================
def reciprocal_rank_fusion(results, k=60):
    scores = {}
    doc_map = {}

    for source_docs in results:
        for rank, doc in enumerate(source_docs):
            doc_id = doc.metadata.get("article_key", doc.page_content[:20])
            if doc_id not in scores:
                scores[doc_id] = 0
                doc_map[doc_id] = doc
            scores[doc_id] += 1 / (k + rank)

    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    return [doc_map[doc_id] for doc_id, _ in ranked]

def get_relevant_docs(query: str) -> list[Document]:
    faiss_retriever, bm25_retriever = initialize_retrievers()

    legal_query = optimize_query_for_legal_search(query)

    bm25_docs = bm25_retriever.invoke(legal_query)
    faiss_docs = faiss_retriever.invoke(query)

    fused_docs = reciprocal_rank_fusion([bm25_docs, faiss_docs])[:20]

    if co and fused_docs:
        try:
            rerank = co.rerank(
                model="rerank-multilingual-v3.0",
                query=query,
                documents=[d.page_content for d in fused_docs],
                top_n=5
            )
            return [fused_docs[r.index] for r in rerank.results]
        except Exception:
            return fused_docs[:5]

    return fused_docs[:5]

# =====================
# Answer Generation
# =====================
def answer_question(question: str) -> dict:
    docs = get_relevant_docs(question)

    if not docs:
        return {
            "answer": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØµÙˆØµ Ù†Ø¸Ø§Ù…ÙŠØ© Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.",
            "articles": []
        }

    # ğŸ”’ Fail-safe: no governing rule â†’ no answer
    if not has_governing_rule(docs):
        return {
            "answer": (
                "Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ù„Ø§ ØªØªØ¶Ù…Ù† Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© Ø§Ù„Ø­Ø§ÙƒÙ…Ø© Ù„Ù„Ù…Ø³Ø£Ù„Ø© "
                "(Ù…Ø«Ù„ Ø§Ù„Ù…Ø§Ø¯Ø© 66 Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª Ø§Ù„Ø´Ø±Ø¹ÙŠØ©)ØŒ "
                "ÙˆØ§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…ØªØ§Ø­Ø© ØªØªØ¹Ù„Ù‚ Ø¨Ø­Ø§Ù„Ø§Øª Ø®Ø§ØµØ© Ù„Ø§ ØªÙƒÙÙŠ Ù„Ù„ÙØµÙ„ ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„."
            ),
            "articles": [d.metadata for d in docs]
        }

    prompt = build_prompt(question, docs)

    response = client.chat.completions.create(
        model=GPT_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0
    )

    return {
        "answer": response.choices[0].message.content.strip(),
        "articles": [d.metadata for d in docs]
    }
