# rag_pipeline.py

import os
import json
import time
import requests

from dotenv import load_dotenv

from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_openai import OpenAIEmbeddings  # Ø¬Ø¯ÙŠØ¯ ğŸ‘ˆ


from openai import OpenAI

load_dotenv()

# ==========================
# 0) Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
# ==========================

MODEL_BACKEND = os.getenv("MODEL_BACKEND", "openai").lower()
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4.1")
HF_TOKEN = os.getenv("HF_TOKEN")

ALLAM_MODEL_ID = os.getenv("ALLAM_MODEL_ID", "SDAIA-ALLEM/allam-2-9b-instruct")
JAIS_MODEL_ID = os.getenv("JAIS_MODEL_ID", "inceptionai/jais-30b-v3")

# Ø¹Ù…ÙŠÙ„ OpenAI (ÙŠÙØ³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† MODEL_BACKEND = openai)
openai_client = OpenAI()  # ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ OPENAI_API_KEY ÙÙŠ Ø§Ù„Ù€ .env


# ==========================
# 1) Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¹Ø§Ù…Ø©
# ==========================

AR_DIGITS = "Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©"
EN_DIGITS = "0123456789"
DIGIT_TRANS = str.maketrans(AR_DIGITS, EN_DIGITS)


def normalize_numbers(text: str) -> str:
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ (Ù…Ø«Ù„ Ù©Ù /Ù¢ â†’ 90/2)."""
    if not isinstance(text, str):
        return text
    return text.translate(DIGIT_TRANS)


def debug_print(msg: str):
    """Ù„Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ° (ØªÙ‚Ø¯Ø± ØªØ¹Ø·Ù„Ù‡Ø§ Ù„Ø§Ø­Ù‚Ù‹Ø§)."""
    print(f"[RAG-DEBUG] {msg}")


# ==========================
# 2) Ø¥Ø¹Ø¯Ø§Ø¯ Embeddings Ø¹Ø±Ø¨ÙŠØ©
# ==========================

def load_arabic_embeddings():
    """
    Ù†Ø³ØªØ®Ø¯Ù… OpenAI Embeddings (text-embedding-3-small)
    Ø¹Ø´Ø§Ù† Ù†ØªÙØ§Ø¯Ù‰ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ HuggingFace Ø«Ù‚ÙŠÙ„ Ø¯Ø§Ø®Ù„ Render.
    """
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    return OpenAIEmbeddings(
        model="text-embedding-3-small"
        # Ù…Ù…ÙƒÙ† ØªØ¶ÙŠÙ dimensions Ø¥Ø°Ø§ Ø­Ø§Ø¨ØŒ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…Ù…ØªØ§Ø²:
        # dimensions=1536
    )




# ==========================
# 3) ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
# ==========================

def load_legal_documents():
    """
    ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø£Ù†Ø¸Ù…Ø© (JSON) ÙˆØ¨Ù†Ø§Ø¤Ù‡ ÙƒÙ…Ø³ØªÙ†Ø¯Ø§Øª LangChain.
    ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø± Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ù…ØªØºÙŠØ±:
    SYSTEMS_DATASET_PATH
    """
    dataset_path = os.getenv("SYSTEMS_DATASET_PATH", "full_systems_dataset_fixed.json")

    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"Legal systems dataset not found at: {dataset_path}")

    with open(dataset_path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)

    # ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø§Ù„Ø§Øª ÙŠÙƒÙˆÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ù‚Ø§Ø¦Ù…Ø© Ù‚ÙˆØ§Ø¦Ù…
    if isinstance(raw_data, list) and raw_data and isinstance(raw_data[0], list):
        data = [item for sub in raw_data for item in sub]
    else:
        data = raw_data

    documents = []
    for item in data:
        if not isinstance(item, dict):
            continue

        system = item.get("system", "")
        article_number = normalize_numbers(str(item.get("article_number", "")))
        text = item.get("text", "")

        page_content = f"{system} - Ø§Ù„Ù…Ø§Ø¯Ø© {article_number}: {text}"
        metadata = {
            "system": system,
            "article_number": article_number,
            "original_text": text,
        }
        documents.append(Document(page_content=page_content, metadata=metadata))

    debug_print(f"Loaded {len(documents)} documents into RAG.")
    return documents


# ==========================
# 4) Ø­Ø§Ù„Ø© RAG (Ù†Ø¨Ù†ÙŠÙ‡Ø§ Ø£ÙˆÙ„ Ù…Ø±Ø© ÙÙ‚Ø·)
# ==========================

class RagState:
    """
    Ù‡Ø°Ù‡ Ø§Ù„ÙƒÙ„Ø§Ø³ Ù…Ø³Ø¤ÙˆÙ„Ø© Ø¹Ù†:
    - ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù€ embeddings
    - ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
    - Ø¨Ù†Ø§Ø¡ FAISS + BM25
    """

    def __init__(self):
        t0 = time.time()
        debug_print("Initializing RAGState (embeddings + documents + indexes)...")

        # 1) Embeddings
        debug_print("Loading embeddings...")
        self.embeddings = load_arabic_embeddings()

        # 2) Legal documents
        debug_print("Loading legal documents...")
        self.documents = load_legal_documents()

        # 3) FAISS
        debug_print("Building FAISS index...")
        vector_store = FAISS.from_documents(self.documents, self.embeddings)
        self.faiss_retriever = vector_store.as_retriever(search_kwargs={"k": 10})

        # 4) BM25
        debug_print("Building BM25 retriever...")
        self.bm25_retriever = BM25Retriever.from_documents(self.documents)
        self.bm25_retriever.k = 10

        debug_print(f"RAG pipeline initialized in {time.time() - t0:.2f} seconds.")


# Ø³Ù†Ø®Ø²Ù† Ø§Ù„Ø­Ø§Ù„Ø© Ù‡Ù†Ø§ (ÙƒØ³Ù†Ø¬Ù„ØªÙˆÙ† Ø¨Ø³ÙŠØ·)
_rag_state: RagState | None = None


def get_rag_state() -> RagState:
    """
    Ù†Ø³ØªØ¯Ø¹ÙŠÙ‡Ø§ Ù…Ù† answer_question.
    Ø¥Ø°Ø§ ÙƒØ§Ù†Øª None â†’ Ù†Ù†Ø´Ø¦ RAGState Ù„Ø£ÙˆÙ„ Ù…Ø±Ø©.
    """
    global _rag_state
    if _rag_state is None:
        debug_print("First call detected, creating RagState...")
        _rag_state = RagState()
    return _rag_state


# ==========================
# 5) Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ† FAISS + BM25
# ==========================

def hybrid_retrieve(query: str, k: int = 5):
    """
    Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù‡Ø¬ÙŠÙ†:
    - BM25 (ÙˆØ²Ù† 2.0)
    - FAISS/Embeddings (ÙˆØ²Ù† 1.0)
    ÙˆÙ†Ø®ØªØ§Ø± Ø£Ø¹Ù„Ù‰ k Ù…ÙˆØ§Ø¯ Ø¨Ø¹Ø¯ Ø§Ù„Ø¯Ù…Ø¬.
    """
    t0 = time.time()
    query_norm = normalize_numbers(query)

    state = get_rag_state()

    docs_bm25 = state.bm25_retriever.get_relevant_documents(query_norm)
    docs_faiss = state.faiss_retriever.get_relevant_documents(query_norm)

    combined = {}

    # Ø¥Ø¹Ø·Ø§Ø¡ ÙˆØ²Ù† Ø£Ø¹Ù„Ù‰ Ù„Ù€ BM25 Ù„Ø£Ù†Ù‡ Ù…Ù…ØªØ§Ø² Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ©
    for d in docs_bm25:
        key = (d.metadata.get("system"), d.metadata.get("article_number"))
        if key not in combined:
            combined[key] = (d, 2.0)

    # Embeddings ÙƒØ¯Ø¹Ù… Ø¯Ù„Ø§Ù„ÙŠ
    for d in docs_faiss:
        key = (d.metadata.get("system"), d.metadata.get("article_number"))
        if key not in combined:
            combined[key] = (d, 1.0)

    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ÙˆØ²Ù† (ØªÙ†Ø§Ø²Ù„ÙŠÙ‹Ø§)
    sorted_docs = sorted(combined.values(), key=lambda x: -x[1])
    top_docs = [d[0] for d in sorted_docs][:k]

    debug_print(
        f"Hybrid retrieve: BM25={len(docs_bm25)}, FAISS={len(docs_faiss)}, "
        f"merged={len(sorted_docs)}, returned={len(top_docs)}, "
        f"time={time.time() - t0:.3f}s"
    )

    return top_docs


# ==========================
# 6) ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¥Ù„Ù‰ Ù…ÙˆØ§Ø¯ + Ù…Ø±Ø§Ø¬Ø¹
# ==========================

def docs_to_articles_with_refs(docs):
    """
    Ù†Ø­ÙˆÙ„ Ù…Ø³ØªÙ†Ø¯Ø§Øª LangChain Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…ÙˆØ§Ø¯ + Ù…Ø±Ø§Ø¬Ø¹ Ù†ØµÙŠØ© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ Ø§Ù„Ù€ prompt
    """
    articles = []
    refs = []
    for i, doc in enumerate(docs, start=1):
        ref = f"[{i}]"
        meta = doc.metadata or {}
        system = meta.get("system", "")
        article_number = meta.get("article_number", "")
        text = meta.get("original_text", doc.page_content)

        articles.append(
            {
                "ref": ref,
                "system": system,
                "article_number": article_number,
                "text": text,
            }
        )
        refs.append(f"{ref} {system} â€“ {article_number}")
    return articles, refs


# ==========================
# 7) Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ Prompt Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ
# ==========================

def build_teacher_prompt(question: str, articles, refs_list):
    articles_text = ""
    for a in articles:
        articles_text += f"{a['ref']} {a['system']} â€“ {a['article_number']}\n"
        articles_text += f"Ø§Ù„Ù†Øµ:\n{a['text']}\n\n"

    refs_text = "\n".join(refs_list)

    prompt = f"""
Ø£Ù†Øª Ù†Ø¸Ø§Ù… Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© (Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ©ØŒ Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§ØªØŒ Ø§Ù„Ø¥Ø«Ø¨Ø§ØªØŒ Ø§Ù„Ø¥ÙÙ„Ø§Ø³ ÙˆÙ„ÙˆØ§Ø¦Ø­Ù‡Ø§ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØ©).

Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø© Ø£Ù…Ø§Ù…Ùƒ (Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ø´ÙŠØ¡ Ù…Ù† Ø®Ø§Ø±Ø¬Ù‡Ø§):

{articles_text}

Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ØªØ§Ø­Ø©:
{refs_text}

Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:
{question}

Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:
- Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙˆØ¨Ø£Ø³Ù„ÙˆØ¨ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ù†Ø¸Ù….
- Ø§Ø³ØªÙ†Ø¯ ÙÙ‚Ø· Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© Ø£Ø¹Ù„Ø§Ù‡.
- Ø¹Ù†Ø¯Ù…Ø§ ØªØ³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ù…Ø§Ø¯Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù‚Ù… Ø§Ù„Ù…Ø±Ø¬Ø¹ Ø¨ÙŠÙ† Ù‚ÙˆØ³ÙŠÙ† Ù…Ø«Ù„ [1] Ø£Ùˆ [2].
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…ÙˆØ§Ø¯ ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¬Ø§Ø²Ù…Ø©ØŒ ÙˆØ¶Ù‘Ø­ Ø°Ù„Ùƒ ØµØ±Ø§Ø­Ø©.

Ø§ÙƒØªØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø¢Ù†:
"""
    return prompt.strip()


# ==========================
# 8) Ø¯ÙˆØ§Ù„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù€ LLMs
# ==========================

def call_openai_teacher(prompt: str) -> str:
    """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ OpenAI (Ù…Ø«Ù„Ø§Ù‹ GPT-4.1)"""
    resp = openai_client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
        max_tokens=2000,
    )
    return resp.choices[0].message.content.strip()


def _call_hf_text_gen(prompt: str, model_id: str) -> str:
    """
    Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Hugging Face Inference API
    (ÙŠØ³ØªØ®Ø¯Ù… Ù…Ø¹ Allam / Jais).
    """
    if not HF_TOKEN:
        raise RuntimeError("HF_TOKEN is not set in environment variables.")

    url = f"https://api-inference.huggingface.co/models/{model_id}"
    headers = {
        "Authorization": f"Bearer {HF_TOKEN}",
        "Content-Type": "application/json",
    }
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": 2000,
            "temperature": 0.2,
            "return_full_text": False,
        },
    }

    resp = requests.post(url, headers=headers, json=payload, timeout=120)
    resp.raise_for_status()
    data = resp.json()

    # ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ø´Ø§Ø¦Ø¹Ø© Ù„Ù†ØªØ§Ø¦Ø¬ HF
    if isinstance(data, list) and data:
        first = data[0]
        if isinstance(first, dict):
            text = first.get("generated_text") or first.get("summary_text")
            if isinstance(text, str):
                return text.strip()

    if isinstance(data, dict):
        text = data.get("generated_text") or data.get("summary_text")
        if isinstance(text, str):
            return text.strip()

    raise RuntimeError(f"Unexpected HF response: {data}")


def call_allam_teacher(prompt: str) -> str:
    """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Allam Ø¹Ø¨Ø± HF API"""
    return _call_hf_text_gen(prompt, ALLAM_MODEL_ID)


def call_jais_teacher(prompt: str) -> str:
    """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Jais Ø¹Ø¨Ø± HF API"""
    return _call_hf_text_gen(prompt, JAIS_MODEL_ID)


def call_teacher_llm(prompt: str) -> str:
    """
    ÙŠØ®ØªØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ MODEL_BACKEND:
    - openai
    - allam
    - jais
    """
    backend = (MODEL_BACKEND or "openai").lower()
    debug_print(f"Using backend: {backend}")

    if backend == "allam":
        return call_allam_teacher(prompt)
    if backend == "jais":
        return call_jais_teacher(prompt)
    # Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
    return call_openai_teacher(prompt)


# ==========================
# 9) Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙŠ ÙŠØ³ØªØ¯Ø¹ÙŠÙ‡Ø§ API.py
# ==========================

def answer_question(question: str):
    """
    Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ø§Ù„ØªÙŠ ÙŠØ³ØªØ¯Ø¹ÙŠÙ‡Ø§ API.py:
    - ØªØ³ØªØ±Ø¬Ø¹ Ø§Ù„Ù…ÙˆØ§Ø¯
    - ØªØ¨Ù†ÙŠ prompt
    - ØªØ³ØªØ¯Ø¹ÙŠ LLM
    - ØªØ¹ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© + Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©
    """
    docs = hybrid_retrieve(question, k=5)

    if not docs:
        return {
            "answer": "Ù„Ù… Ø£Ø¬Ø¯ Ù…ÙˆØ§Ø¯ Ù†Ø¸Ø§Ù…ÙŠØ© ÙƒØ§ÙÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.",
            "articles": [],
        }

    articles, refs = docs_to_articles_with_refs(docs)
    prompt = build_teacher_prompt(question, articles, refs)
    answer = call_teacher_llm(prompt)

    return {
        "answer": answer,
        "articles": articles,
    }
